---
title: 统计学习初探
date: 2018-03-12 14:05:15
tags:
- statistical learning
- 统计学习
mathjax: true
---
本文不是一份完整 & 系统的统计学习入门指南. 笔者选择了三个简单的统计学习方法, 意在通过蜻蜓点水式的介绍, 让工程师对于统计学习有一些初步的感性认知.

在统计学习的各个分支, `Python` 语言是工业界目前的事实标准之一. 所以后文的例子也将通过 `Python` 来实现. ([仓库地址](https://github.com/XanthosisJYW/head-first-statistical-learning))

因为本文涉及的内容极其有限, 对于想要进一步深入统计学习的读者, 可以考虑阅读 [*统计学习导论*](https://book.douban.com/subject/26430936/) 一书. 这本书降低了数学上的门槛, 对于抱着应用目的的读者更为友善.

## 1. 线性回归及其他

线性回归 (Linear Regression) 是大部分统计学习教材的入门第一课, 那到底什么是线性回归呢?

简单来说, 所有的统计学习都希望通过学习这一手段来估计 / 推断 / 拟合相应的模型. 而线性回归的模型便是以线性的表达式为基础的.

这么说比较抽象, 我们来看一个例子, 这里的数据反映的是不同媒介上的广告投入与最终销量之间的关系.

![Relation between Sales and TV Ads](/figures/sales_tv.png)

上图中, 蓝色点是原始数据, 表示电视广告的投放与销量之间的关系. 而红色直线是计算得出的线性模型. 在 2 维平面上, 直线所表达的就是线性关系.

![Relation between Sales and TV & Radio Ads](/figures/sales_tv_radio.png)

更进一步, 对于高维空间 (这里以 3 维为例子), 蓝色点表示的是电视 & 广播广告与销量之间的关系. 而浅红色的平面则是计算得出的线性模型. 在 3 维空间中, 平面所表达的也是一种线性关系.

### 1.1 线性回归的形式

线性回归的模型可以表示成如下的一般形式,

$$
Y = \alpha X + \beta
$$

其中, $X$, $Y$ 和 $\beta$ 可以是向量, 而 $\alpha$ 可以是矩阵.

但目前, 为了简单起见, 我们将该式子直接视为一元的表达式. 而我们所要做的就是通过已知的 $X$ 和 $Y$, 求得 $\alpha$ 和 $\beta$ 的值.

只要得到 $\alpha$ 和 $\beta$ 的值, 我们就可以复原整个表达式. 进而得到 $X$ 和 $Y$ 之间明确的关系.

### 1.2 线性回归的方法

上一节提到了, 我们想要知道 $\alpha$ 和 $\beta$ 的值. 为此, 我们常使用 **最小二乘法 (Least Squares)**. 具体的数学细节不在此展开, 可以参阅如下 [Wiki 词条](https://www.wikiwand.com/zh-cn/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95) 获得进一步的信息.

即便不知道具体的数学细节, 我们也可以通过 Python 库的既有方法来计算结果. 下面, 我们就以本文的第一张图为例, 看下计算的结果.

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  Sales   R-squared:                       0.612
Model:                            OLS   Adj. R-squared:                  0.610
Method:                 Least Squares   F-statistic:                     312.1
Date:                Sun, 11 Mar 2018   Prob (F-statistic):           1.47e-42
Time:                        16:56:16   Log-Likelihood:                -519.05
No. Observations:                 200   AIC:                             1042.
Df Residuals:                     198   BIC:                             1049.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      7.0326      0.458     15.360      0.000       6.130       7.935
TV             0.0475      0.003     17.668      0.000       0.042       0.053
==============================================================================
Omnibus:                        0.531   Durbin-Watson:                   1.935
Prob(Omnibus):                  0.767   Jarque-Bera (JB):                0.669
Skew:                          -0.089   Prob(JB):                        0.716
Kurtosis:                       2.779   Cond. No.                         338.
==============================================================================
```

其中 `Intercept` 表示截距, 也就是 $\beta$ 的值为 `7.0326`, 而 `TV` 对应电视的系数, 也就是 $\alpha$ 的值为 `0.0475`. 通过这两个值, 我们就可以还原一条直线. 也就是第一幅图中的红色直线. 同时也可以进一步利用模型进行预测.

原则上, 我们只需要这两个值就够了. 但结果中还有一个值需要留意, 也就是 `P>|t|`. 简单来说, 这个值表示我们获得参数靠不靠谱, 值越接近 0 越靠谱.

### 1.3 多元线性回归

因为例子中的数据其实是高维的, 我们有理由猜测, 可能一元 (单变量) 的形式不足以完整反映模型的实质. 下面, 我们看一看把电视, 广播 和 报纸统统纳入分析的结果.

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  Sales   R-squared:                       0.897
Model:                            OLS   Adj. R-squared:                  0.896
Method:                 Least Squares   F-statistic:                     570.3
Date:                Sun, 11 Mar 2018   Prob (F-statistic):           1.58e-96
Time:                        16:56:18   Log-Likelihood:                -386.18
No. Observations:                 200   AIC:                             780.4
Df Residuals:                     196   BIC:                             793.6
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      2.9389      0.312      9.422      0.000       2.324       3.554
TV             0.0458      0.001     32.809      0.000       0.043       0.049
Radio          0.1885      0.009     21.893      0.000       0.172       0.206
Newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011
==============================================================================
Omnibus:                       60.414   Durbin-Watson:                   2.084
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241
Skew:                          -1.327   Prob(JB):                     1.44e-33
Kurtosis:                       6.332   Cond. No.                         454.
==============================================================================
```

这里我们可以看到, `Newspaper` 所对应的 `P>|t|` 高的离谱. 实际上, 报纸对销量所起的作用及其有限, 因为报纸和广播的相关性极大.

通过下面两张图可以非常直观的看出问题.

![Correlation between Radio and TV](/figures/radio_tv_correlation.png)

从这张图我们可以看出, 电视和广播之间没有太大的关联.

![Correlation between Radio and Newspaper](/figures/radio_newspaper_correlation.png)

但这张图则不同, 可以看到广播和报纸之间有着不小的关联.

这部分的关联也可以通过相关系数矩阵发现.

```
           Newspaper     Radio        TV
Newspaper   1.000000  0.354104  0.056648
Radio       0.354104  1.000000  0.054809
TV          0.056648  0.054809  1.000000
```

### 1.4 线性回归的局限

到这里, 我们对于线性回归的介绍就告一段落了. 线性回归模型简单, 易解释的特点使他成为了多数此类教材的首选知识点.

但线性回归也有着自己的局限, 那就是不适用于分类. 我们可以在获得各个系数之后对给定的输入预测输出, 但我们很难通过输出直接判定结果的分类.

而逻辑斯谛回归 (Logistic Regression) 则很好的胜任了分类的任务, 感兴趣的可以自行查阅相关的材料.

## 2. 决策树

决策树 (Decition Tree) 也是一种非常常用的模型. 这里我们主要介绍决策树中的分类树.

顾名思义, 分类树就是通过树状的条件判断, 最终决定输入的分类. 转换成代码就是一系列 `IF-ELSE` 判断.

下图是一个基于天平的例子. 主要有四个变量, 天平左右两端的重量, 左右两臂的距离. 这些变量最终决定了天平是向左 / 右倾斜, 还是平衡.

![Correlation between Radio and TV](/figures/decision_tree.png)

可以从图中看出, 每个节点会对其中一个属性做条件判断, 然后分流如下一级的节点. 最后在叶子节点确定最终的分类.

### 2.1 分类方法

分类的方法很多, 常见的可以通过如下方法分类:

- 错误率
- 基尼系数 (Gini Index)
- 信息熵 (Information Entropy)

这里我们简要介绍基于信息熵的分类方法. 关于信息熵的背景信息, 可以查阅如下的 [Wiki 词条](https://www.wikiwand.com/zh-cn/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)).

先给出信息熵的公式

$$
H = -\sum p_k (x) \log_2 p_k(x)
$$

其中 $p_k (x)$ 是类别为 $k$ 的样本在当前数据中的比例.

假设我们如下的已知关于猕猴桃好不好吃的数据, 他们的信息熵该怎么计算呢?

```
 编号 | 甜    | 涩    | 分类
 --- | ----- | ----- | -----
 1   | True  | True  | 不好吃
 2   | True  | False | 好吃
 3   | True  | False | 好吃
 4   | False | True  | 不好吃
 5   | False | True  | 不好吃
```

由公式可知, 目前的信息熵是

$$
\- \frac{2}{5} \log\_2 \frac{2}{5} \- \frac{3}{5} \log\_2 \frac{3}{5} = 0.97
$$

那下一步我们以哪个属性作为分类基准比较好呢? 如果以甜不甜作为分类依据, 则甜的猕猴桃的信息熵为

$$
\- \frac{1}{3} \log\_2 \frac{1}{3} \- \frac{2}{3} \log\_2 \frac{2}{3} = 0.91
$$

不甜的猕猴桃信息熵为

$$
\- \frac{0}{2} \log\_2 \frac{0}{2} - \frac{2}{2} \log\_2 \frac{2}{2} = 0
$$

合起来总的信息熵为

$$
\frac{3}{5} \* 0.91 + \frac{2}{5} \* 0 = 0.55
$$

那么以甜不甜作为分类基准, 我们获得了

$$
0.97 - 0.55 = 0.42
$$

的信息熵增益.

同样的, 我们可以以涩不涩为分类基准, 我们最终将获得

$$
0.97 - 0.0 = 0.97
$$

的信息熵增益.

如此看来, 我们以涩不涩作为第一次分类的标准非常合适. 以此类推到后续的分类, 乃至更更为复杂的分类状况, 如天平的例子, 我们一样可以完成分类.

决策树的例子就讲到这里. 事实上, 决策树还有其他的变种, 感兴趣的可以自行检索.

## 3. 主成分分析 (PCA)

最后要介绍的是主成分分析 (Principal Components Analysis), 这是一种在数据科学 / 统计学习中非常常用的技术. 为了避免困扰, 这部分我不会提及任何数学公式.

现实世界中, 采集回来的数据往往是高维的. 而机器的算力往往不足以支持这么多维度的数据. 面对这样的状况, 我们自然而然想到了降维, 因为有些维度上的数据并不重要.

如何判定哪些维度是主要的成分呢? 我们来看一个 2 维的例子.

![Original Data](/figures/pca_01.png)

对于上图中的数据点, 我们可以找到数据离散程度最大的方向, 进而确定其他与之垂直的方向 (图中的黑色箭头).

![Rotated Data](/figures/pca_02.png)

根据刚才获得的方向, 我们可以对原始的数据进行旋转, 这样我们就获得了图 2. 

假设我们的机器只能处理 1 维的数据, 那我们就可以选择横向的数据, 进而舍弃纵向的数据, 因为相比之下, 前者更为体现了更多的信息, 更为重要.

所谓的主成分分析就是通过分析不同维度的重要性, 为计算者筛选维度提供有力的支撑. 至于完更为详尽的相关内容, 可以查阅如下的 [Wiki 词条](https://www.wikiwand.com/zh-cn/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90) 作进一步了解.

## 4. 后记

如本文开头所说的, 我们只是蜻蜓点水式的介绍了几个最为基本的统计学习模型. 如果对于上述的内容还有无法理解的内容, 可以去仔细阅读上文提到的 [*统计学习导论*](https://book.douban.com/subject/26430936/) 一书.

如果对于 [*统计学习导论*](https://book.douban.com/subject/26430936/) 中提到的内容, 尤其是涉及统计知识的部分感到困惑, 可以去学习 [Ruibin Xi 老师的统计课件](http://www.math.pku.edu.cn/teachers/xirb/Courses/statprobB/statprobB2017.html), 内容足够详尽, 且足够平易近人.